{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a0a798",
   "metadata": {},
   "source": [
    "# 1D Harmonic Oscillator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb0335d",
   "metadata": {},
   "source": [
    "Here we will solve the Harmonic Oscillator (HO) in 1D for a single particle. The Hamiltonian reads, in HO units:\n",
    "\n",
    "$$\\hat{H}=-\\frac{1}{2}\\frac{d²}{dx²}+\\frac{1}{2}mw²x²$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa408db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from IPython import display\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Hardware device (CPU or GPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998e5043",
   "metadata": {},
   "source": [
    "## General parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b262b8e",
   "metadata": {},
   "source": [
    "We now define the various parameters and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e509882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network hyperparameters\n",
    "Nin = 1   # Inputs to the neural network\n",
    "Nout = 1  # Outputs of the neural network\n",
    "Nhid = 4  # Nodes in the hidden layer\n",
    "\n",
    "# Network parameters.\n",
    "seed = 1\n",
    "W1 = torch.rand(Nhid, Nin, requires_grad=True) * (-1.) # First set of coefficients. Shape = (nodes, Nin)\n",
    "B = torch.rand(Nhid, requires_grad=True) * 2. - 1.    # Set of bias parameters\n",
    "W2 = torch.rand(Nout, Nhid, requires_grad=True)        # Second set of coefficients. Shape = (Nout, nodes)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Training hyperparameters\n",
    "epochs = 3000\n",
    "lr = 2e-2  # Learning rate\n",
    "\n",
    "# Mesh parameters\n",
    "n_samples = 100                    # Mesh division\n",
    "train_a = -8                      # Mesh lower limit\n",
    "train_b = 8                        # Mesh upper limit\n",
    "h = (train_b - train_a)/(n_samples-1)  # Mesh parameter \"h\"\n",
    "Q_train = torch.linspace(train_a, train_b, n_samples, \n",
    "                         requires_grad=True, \n",
    "                         device=device)  \n",
    "\n",
    "target = (1/np.pi)**(1/4) * torch.exp(-Q_train.pow(2)/2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d83f17",
   "metadata": {},
   "source": [
    "## The neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b342a",
   "metadata": {},
   "source": [
    "We now create the neural network. In PyTorch every neural network must be created as a child of the ```nn.Module``` class. Inside the ```__init__``` method we define the layers. In our case we have a single hidden layer, and therefore a single activation function and two linear transformations (one at each \"side\" of the hidden layer). Then, inside the block ```with torch.no_grad():``` we define the initial parameters of the network.\n",
    "\n",
    "Finally, inside the ```forward``` method we set the connections between the different elements of the network, this is, we set the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "049eb6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarmonicNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HarmonicNN, self).__init__()\n",
    "        \n",
    "        # We set the operators \n",
    "        self.lc1 = nn.Linear(in_features=Nin, \n",
    "                             out_features=Nhid, \n",
    "                             bias=True)   # shape = (Nhid, Nin)\n",
    "        self.actfun = nn.Sigmoid()        # activation function\n",
    "        self.lc2 = nn.Linear(in_features=Nhid, \n",
    "                             out_features=Nout, \n",
    "                             bias=False)  # shape = (Nout, Nhid)\n",
    "        \n",
    "        # We set the parameters \n",
    "        with torch.no_grad():\n",
    "            self.lc1.weight = nn.Parameter(W1)\n",
    "            self.lc1.bias = nn.Parameter(B)\n",
    "            self.lc2.weight = nn.Parameter(W2)\n",
    "   \n",
    "    # We set the architecture\n",
    "    def forward(self, x): \n",
    "        o = self.lc2(self.actfun(self.lc1(x)))\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df1e97d",
   "metadata": {},
   "source": [
    "The network we have just created has the following graph representation:\n",
    "\n",
    "<img src=\"simple_ann.PNG\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: center;\" \n",
    "     width=\"500\"\n",
    "     height=\"400\"/>\n",
    "     \n",
    "We can check that our code is actually producing this neural network with the following syntax: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbaf70a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN architecture:\n",
      " HarmonicNN(\n",
      "  (lc1): Linear(in_features=1, out_features=4, bias=True)\n",
      "  (actfun): Sigmoid()\n",
      "  (lc2): Linear(in_features=4, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = HarmonicNN().to(device)\n",
    "print(\"NN architecture:\\n\", net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c654b",
   "metadata": {},
   "source": [
    "## The loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d91dc3",
   "metadata": {},
   "source": [
    "Now we must define the loss function, which in our case is the energy expectation computed with the HO Hamiltonian and our NQS. This is,\n",
    "$$\\newcommand{\\bra}[1]{{ \\left\\langle #1 \\right|} }\n",
    "\\newcommand{\\ket}[1]{{ \\left|  #1 \\right\\rangle } }\n",
    "\\mathcal{L}=\\frac{\\bra{\\psi_\\theta}\\hat{H}\\ket{\\psi_\\theta}}{\\bra{\\psi_\\theta}\\psi_\\theta\\rangle}. $$\n",
    "\n",
    "This expression can be evaluated as two integrals in position space:\n",
    "\n",
    "$$\\bra{\\psi_\\theta}\\hat{H}\\ket{\\psi_\\theta}=\\int_{-\\infty}^\\infty \\psi\\hat{H}\\psi dx$$\n",
    "\n",
    "$$ \\bra{\\psi_\\theta}\\psi_\\theta\\rangle=\\int_{-\\infty}^\\infty \\psi² dx$$\n",
    "\n",
    "The first integral can be rewritten so that the second derivative does not appear. This will be useful when computing the loss function. The \"trick\" is to integrate the kinetic term by parts and impose that \n",
    "\n",
    "$$ \\lim_{x\\to -\\infty}\\psi(x) = \\lim_{x\\to +\\infty}\\psi(x) = 0. $$\n",
    "\n",
    "After this step we can discretize the integrals according to our lattice:\n",
    "\n",
    "$$ \\bra{\\psi_\\theta}\\hat{H}\\ket{\\psi_\\theta}=\\frac{1}{2}\\int[(\\frac{d\\psi_\\theta(x)}{dx})²+x²\\psi_\\theta²(x)]dx \\approx \\frac{1}{2}\\sum_{i=1}^{N_x} w_i [(\\frac{d\\psi_\\theta(x_i)}{dx})²+x²\\psi_\\theta²(x_i)], $$\n",
    "\n",
    "$$\\bra{\\psi_\\theta}\\psi_\\theta\\rangle\\approx \\sum_{i=1}^{N_x} w_i \\psi_\\theta^2(x_i)\\equiv N.$$\n",
    "\n",
    "We now define the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c37b9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost():\n",
    "# VECTORIZED VERSION OF FIRST AND SECOND DERIVATIVES    \n",
    "    psi = net(X)    \n",
    "    pis2 = psi.pow(2)\n",
    "    dpsi_dx, = grad(outputs=psi, \n",
    "                    inputs=X,\n",
    "                    grad_outputs=torch.ones_like(psi), \n",
    "                    create_graph=True)            \n",
    "    \n",
    "    N = h * torch.sum(psi2)                        \n",
    "    U = h * (1/2) * torch.sum(X.pow(2) * psi2) / N  \n",
    "    K = h * (1/2) * torch.sum(dpsi_dx.pow(2)) / N              \n",
    "    E = U + K   \n",
    "    \n",
    "    return E, U, K, psi, psi / torch.sqrt(N) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f93b0c",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac66f49",
   "metadata": {},
   "source": [
    "We set the function that does the plots periodically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "251df4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pic(i, x, psi, psi_normalized, loss, U, K):\n",
    "    \n",
    "    # Overlap\n",
    "    ax1.set_title(\"Epoch {}\".format(i+1))\n",
    "    wf = psi_normalized.detach().numpy()\n",
    "    if np.max(wf) < 0: wf = -wf\n",
    "    ax1.plot(x.detach().numpy(), wf, label='$\\psi_\\mathrm{NN}$')\n",
    "    ax1.plot(x.detach().numpy(), target.detach().numpy(), linestyle=\"--\", \n",
    "             label='$\\psi_\\mathrm{target}$')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Wave function\n",
    "    ax2.set_title(\"Epoch {}\".format(i+1))\n",
    "    ax2.plot(x.detach().numpy(), psi.detach().numpy(), \n",
    "             label='$\\psi_\\mathrm{NQS/envelope}$')\n",
    "    ax2.legend()\n",
    "\n",
    "    # Loss\n",
    "    ax3.set_title(\"Loss Function\")\n",
    "    ax3.plot(np.linspace(1, i+1, i+1), loss, label='$E$')\n",
    "    ax3.plot(np.linspace(1, i+1, i+1), U, label='$U$')\n",
    "    ax3.plot(np.linspace(1, i+1, i+1), K, label='$K$')\n",
    "    ax3.plot(np.linspace(1, i+1, i+1), np.ones(i+1)-0.5, \n",
    "             linestyle='--', label='$E_0$')\n",
    "    ax3.plot(np.linspace(1, i+1, i+1), np.ones(i+1)-0.75, \n",
    "             linestyle='--', label='$U_0, K_0$')\n",
    "    ax3.legend() \n",
    "    \n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9608aaa",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278b0f8",
   "metadata": {},
   "source": [
    "We are now ready to train the neural network. The first part of the code is related to the plots that will be periodically created during the training, and the actual training happens inside the ```for``` loop. The key lines of code are the following.\n",
    "\n",
    "``` optimizer.zero_grad() ``` sets all gradients to zero so as not to accumulate the ones from the previous iteration or epoch.\n",
    "\n",
    "``` loss.backward() ``` computes the gradient of the loss w.r.t. the parameters passed to the optimizer using the backpropagation algorithm. These gradient components are stored inside the ```.grad```attribute of the tensors with the ```requires_grad``` flag set to ```True```.\n",
    "\n",
    "``` optimizer.step() ``` updates the parameters $\\theta$ making use of the gradient information just computed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(9, 5))\n",
    "ax1, ax2, ax3 = ax[0], ax[1], ax[2]\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "# Overlap\n",
    "ax1.set_xlabel(\"$x$\")\n",
    "ax1.set_ylabel(\"$\\psi(x)$\")\n",
    "\n",
    "# Wave function\n",
    "ax2.set_xlabel(\"$x$\")\n",
    "ax2.set_ylabel(\"$\\{ANN}(x)$\")\n",
    "    \n",
    "# Loss\n",
    "ax3.set_xlabel(\"Epoch\")\n",
    "ax3.set_ylabel(\"Cost\")\n",
    "ax3.set_ylim(0, 2)\n",
    "\n",
    "tot_loss = []\n",
    "tot_U = []\n",
    "tot_K = []\n",
    "X = Q_train.clone().unsqueeze(1)\n",
    "\n",
    "# Training\n",
    "for i in tqdm(range(epochs), desc=\"Loading...\"):\n",
    "    loss, U, K, psi, psi_normalized = cost()    # Energy and \\phi calculation\n",
    "    \n",
    "    optimizer.zero_grad() # initialize gradients to zero at each epoch\n",
    "    loss.backward()       # computation of the gradients\n",
    "    optimizer.step()      # update of the parameters\n",
    "    \n",
    "    # loss, U, K, ph, phi = cost() \n",
    "    loss_accum.append(loss.item())\n",
    "    U_accum.append(U.item())\n",
    "    K_accum.append(K.item())\n",
    "    \n",
    "    if i == epochs - 1 or (i % 100 == 0): # We plot periodically\n",
    "        pic(i, Q_train, psi, psi_normalized, loss_accum, U_accum, K_accum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544b5ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
